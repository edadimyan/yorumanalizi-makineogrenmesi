{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1TDI71Qchp1M6lEtCG6Dv8lyivDzaHs9e","authorship_tag":"ABX9TyMG89oztUO2FuTA9tkj4aTV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!ls ./drive/MyDrive/2024\\ Güz\\ Dönemi/veri\\ mühendisliği/"],"metadata":{"id":"Qy6I2U6Eigmp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734731817526,"user_tz":-60,"elapsed":202,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"7b584bd6-b02b-41e3-d436-453fcf084e96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'İlerleme Raporu Sosyal Medya.gdoc'   yorumlar.gsheet  'yorumlar - Sayfa1.csv'\n","'İlerleme Raporu Sosyal Medya.pdf'    yorumlar.ipynb\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Zy3weUVkXo5"},"outputs":[],"source":["from google.colab import files\n","#uploaded = files.upload()\n","\n","\n","import pandas as pd\n","\n","root_dir = \"./drive/MyDrive/2024 Güz Dönemi/veri mühendisliği/\"\n","\n","df = pd.read_csv(root_dir + \"yorumlar - Sayfa1.csv\")\n","df.columns = ['status', 'comment']"]},{"cell_type":"markdown","source":["# Veriyi Hazırlama\n","\n","Veri setimizi model eğitimine uygun hale getirmek için aşağıdaki adımları gerçekleştirmeliyiz:\n","\n","### 1. Metin Temizleme\n","İlk olarak, clean_text fonksiyonu ile her bir metin yorumunu temizliyoruz:\n","* Sentiment analizi yaptığımız için duygunun ifadesine bir katkısı olmadığını düşündüğümüz sayı ve noktalama işaretlerini metinden kaldırıyoruz.\n","* Tüm yorumları küçük harfe dönüştürüp satır başı ve sonundaki boşlukları kaldırıyoruz.\n"],"metadata":{"id":"sdG8vtEQFfjQ"}},{"cell_type":"code","source":["import re\n","def clean_text(text):\n","    text = re.sub(r'\\d+\\.', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text = text.lower()\n","    text = text.strip()\n","\n","    return text\n","\n","\n","df['comment'] = df['comment'].apply(clean_text)"],"metadata":{"id":"VAPg923glgXN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Yorumları Vektörleştirme\n","TF-IDF ve Word2Vec teknikleri kullanılarak her bir yorum bir sayı vektörü haline dönüştürülür.\n","\n","stop_words: Burada, Türkçe durdurma kelimeleri (örneğin, \"ve\", \"ama\", \"bir\" vb.) kullanılmaktadır. Bu kelimeler anlam taşımadığı için modelde dikkate alınmaz. Bu yüzden metinden ayıklanması gerekmektedir."],"metadata":{"id":"W7KMgVQwG3Us"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from gensim.models import Word2Vec\n","\n","# comment sütunundaki metinleri listeye çevir\n","comments = df['comment'].astype(str).tolist()\n","\n","tr_stop_words = ['ve', 'ama', 'bir', 'olarak', 'ile', 'de', 'en', 'çok', 'kadar', 'da', 'için', 'gibi', 'şu', 'bu', 'her', 'herhangi', 'aslında', 'o', 'neden', 'şey', 'ne', 'ya', 'ya da', 'bunu', 'böyle', 'aslında', 'ancak', 'sadece']\n","\n","# 1. TF-IDF ile vektörleştirme\n","tfidf_vectorizer = TfidfVectorizer(max_features=300, stop_words=tr_stop_words)  # max_features ile boyut sınırlanabilir\n","tfidf_matrix = tfidf_vectorizer.fit_transform(comments)\n","\n","# TF-IDF vektörlerini DataFrame'e ekleyelim\n","df['TFIDF_Vector'] = list(tfidf_matrix.toarray())\n","\n","# 2. Word2Vec ile vektörleştirme\n","# Word2Vec için metinleri kelime listelerine ayırmamız gerekiyor\n","tokenized_comments = [comment.split() for comment in comments]\n","\n","# Word2Vec modelini eğit\n","w2v_model = Word2Vec(sentences=tokenized_comments, vector_size=300, window=5, min_count=1, workers=4)\n","\n","# Her yorumu Word2Vec ile vektöre dönüştürmek\n","def get_avg_w2v_vector(sentence):\n","    vectors = [w2v_model.wv[word] for word in sentence if word in w2v_model.wv]\n","    if vectors:  # Eğer kelimeler varsa\n","        return np.mean(vectors, axis=0)\n","    else:  # Eğer kelimeler modelde yoksa\n","        return np.zeros(w2v_model.vector_size)\n","\n","# Her yorumun Word2Vec vektörünü hesapla\n","w2v_vectors = [get_avg_w2v_vector(comment) for comment in tokenized_comments]\n","df['Word2Vec_Vector'] = w2v_vectors\n"],"metadata":{"id":"h2ELyPYxlggt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_copy = df.copy()"],"metadata":{"id":"WNiQOYu41N63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Logistic Regression Modeli"],"metadata":{"id":"owywC5wO7-_z"}},{"cell_type":"markdown","source":["Şimdi hazırlanmış verilerimizi Logistic Regression modeli eğitmek için kullanacağız. TF-IDF vektörlerinin ve Word2Vec vektörlerinin kullanıldığı iki ayrı model eğitip, hangi metin temsil yönteminin sentiment analizinde daha başarılı olduğunu bulmayı amaçlıyorum.\n","\n","### 1. Kategorik Verilerin Encode Edilmesi\n","Çıktı etiketi olan \"status\" sütunu, kategorik bir değişkendir. Dolayısıyla model eğitiminden önce sayısal hale dönüştürülmesi gerekir. LabelEncoder sınıfı, her bir etiketin (örneğin \"Positive\", \"Negative\") sayısal bir değeri karşılık gelmesini sağlar. Bu sayede, makine öğrenmesi modelleri bu etiketleri kullanarak sınıflandırma yapabilir.\n","\n","### 2. Train-Test Split\n","Eğitim ve test veri setlerine ayırma işlemi yapılır. train_test_split fonksiyonu ile verilerin %80'i eğitim, %20'si test seti olarak ayrılır.\n","\n","### 3. Model Eğitimi:\n","İlk model, TF-IDF vektörleri ile eğitilir. Modelin doğruluğu, test verisi kullanılarak tahmin edilip değerlendirilir. Aşağıdaki metrikler raporlanır:\n","* F1-Score\n","* Precision\n","* Recall"],"metadata":{"id":"B-jCMe4eIkRv"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import numpy as np\n","\n","# 1. Label Encoding\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(df['status'])\n","\n","# 2. Bağımsız değişkenler\n","X_tfidf = np.array(df['TFIDF_Vector'].tolist())\n","X_word2vec = np.array(df['Word2Vec_Vector'].tolist())\n","\n","# 3. Veri bölme\n","X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(\n","    X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n","\n","X_word2vec_train, X_word2vec_test, _, _ = train_test_split(\n","    X_word2vec, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# 4. Modelleri tanımlama\n","tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n","word2vec_model = LogisticRegression(max_iter=1000, random_state=42)\n","\n","# 5. TF-IDF Model Eğitimi\n","print(\"TF-IDF Vektörleri ile Model Eğitimi:\")\n","tfidf_model.fit(X_tfidf_train, y_train)\n","y_pred_tfidf = tfidf_model.predict(X_tfidf_test)\n","print(\"Accuracy (TF-IDF):\", accuracy_score(y_test, y_pred_tfidf))\n","print(classification_report(y_test, y_pred_tfidf, zero_division=0))\n","\n","# 6. Word2Vec Model Eğitimi\n","print(\"Word2Vec Vektörleri ile Model Eğitimi:\")\n","word2vec_model.fit(X_word2vec_train, y_train)\n","y_pred_word2vec = word2vec_model.predict(X_word2vec_test)\n","print(\"Accuracy (Word2Vec):\", accuracy_score(y_test, y_pred_word2vec))\n","print(classification_report(y_test, y_pred_word2vec, zero_division=0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyylQYuf73Lt","executionInfo":{"status":"ok","timestamp":1734731824588,"user_tz":-60,"elapsed":657,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"6799cf8e-7ad0-4e60-97ce-b5eb4124da7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Vektörleri ile Model Eğitimi:\n","Accuracy (TF-IDF): 0.875\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.79      0.86       124\n","           1       0.93      0.87      0.90       123\n","           2       0.80      0.95      0.87       153\n","\n","    accuracy                           0.88       400\n","   macro avg       0.89      0.87      0.88       400\n","weighted avg       0.89      0.88      0.88       400\n","\n","Word2Vec Vektörleri ile Model Eğitimi:\n","Accuracy (Word2Vec): 0.365\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       124\n","           1       0.31      0.13      0.18       123\n","           2       0.37      0.85      0.52       153\n","\n","    accuracy                           0.36       400\n","   macro avg       0.23      0.33      0.23       400\n","weighted avg       0.24      0.36      0.25       400\n","\n"]}]},{"cell_type":"markdown","source":["# SVM Modeli\n","\n","Logistic Regression'da kullandığım aynı adımları izledim ve SVM Modeli eğittim."],"metadata":{"id":"zpiHKSw2-ShP"}},{"cell_type":"code","source":["df_2 = df_copy.copy()"],"metadata":{"id":"oUL1l6Jf485D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","# 1. TF-IDF ve Word2Vec vektörlerini numpy array formatına dönüştürme\n","tfidf_vectors = np.array(df_2['TFIDF_Vector'].tolist())  # TFIDF vektörlerini numpy array'e dönüştürme\n","word2vec_vectors = np.array(df_2['Word2Vec_Vector'].tolist())  # Word2Vec vektörlerini numpy array'e dönüştürme\n","\n","# 2. 'status' etiketini alalım ve Label Encoding yapalım\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(df_2['status'])  # Positive, Neutral gibi etiketleri sayısal hale getirme\n","\n","# 3. Veriyi eğitim ve test setlerine ayıralım\n","X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(tfidf_vectors, labels, test_size=0.2, random_state=42)\n","X_train_word2vec, X_test_word2vec, _, _ = train_test_split(word2vec_vectors, labels, test_size=0.2, random_state=42)\n","\n","# ---------------------------- TF-IDF SVM Modeli ----------------------------\n","# 4. TF-IDF vektörleri ile SVM modelini oluşturup eğitelim\n","svm_classifier_tfidf = SVC(kernel='linear')  # Linear kernel kullanıyoruz\n","svm_classifier_tfidf.fit(X_train_tfidf, y_train)\n","\n","# 5. TF-IDF modelinin tahminlerini yapalım\n","y_pred_tfidf = svm_classifier_tfidf.predict(X_test_tfidf)\n","\n","# 6. TF-IDF modelini değerlendirelim\n","print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, y_pred_tfidf))  # Doğruluk oranı\n","print(\"TF-IDF Classification Report:\\n\", classification_report(y_test, y_pred_tfidf, zero_division=0))  # Diğer metrikler\n","\n","# ---------------------------- Word2Vec SVM Modeli ----------------------------\n","# 4. Word2Vec vektörleri ile SVM modelini oluşturup eğitelim\n","svm_classifier_word2vec = SVC(kernel='linear')  # Linear kernel kullanıyoruz\n","svm_classifier_word2vec.fit(X_train_word2vec, y_train)\n","\n","# 5. Word2Vec modelinin tahminlerini yapalım\n","y_pred_word2vec = svm_classifier_word2vec.predict(X_test_word2vec)\n","\n","# 6. Word2Vec modelini değerlendirelim\n","print(\"Word2Vec Model Accuracy:\", accuracy_score(y_test, y_pred_word2vec))  # Doğruluk oranı\n","print(\"Word2Vec Classification Report:\\n\", classification_report(y_test, y_pred_word2vec, zero_division=0))  # Diğer metrikler\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NieXZ16H5CjI","executionInfo":{"status":"ok","timestamp":1734731826443,"user_tz":-60,"elapsed":1857,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"dacedaf7-4250-436a-b8a4-9ffdd83fedac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Model Accuracy: 0.8725\n","TF-IDF Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.90      0.80      0.85       120\n","           1       0.91      0.88      0.90       122\n","           2       0.83      0.92      0.87       158\n","\n","    accuracy                           0.87       400\n","   macro avg       0.88      0.87      0.87       400\n","weighted avg       0.88      0.87      0.87       400\n","\n","Word2Vec Model Accuracy: 0.395\n","Word2Vec Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       120\n","           1       0.00      0.00      0.00       122\n","           2       0.40      1.00      0.57       158\n","\n","    accuracy                           0.40       400\n","   macro avg       0.13      0.33      0.19       400\n","weighted avg       0.16      0.40      0.22       400\n","\n"]}]},{"cell_type":"markdown","source":["# Random Forest Modeli\n","Önceki adımları takip ederek Random Forest eğittim."],"metadata":{"id":"EsHJP6YoAtF8"}},{"cell_type":"code","source":["df_2 = df_copy.copy()"],"metadata":{"id":"2ppGmdQY6RN-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# 1. TF-IDF ve Word2Vec vektörlerini numpy array formatına dönüştürme\n","tfidf_vectors = np.array(df_2['TFIDF_Vector'].tolist())  # TFIDF vektörlerini numpy array'e dönüştürme\n","word2vec_vectors = np.array(df_2['Word2Vec_Vector'].tolist())  # Word2Vec vektörlerini numpy array'e dönüştürme\n","\n","# 2. 'status' etiketini alalım ve Label Encoding yapalım\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(df_2['status'])  # Positive, Neutral gibi etiketleri sayısal hale getirme\n","\n","# 3. Veriyi eğitim ve test setlerine ayıralım\n","X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(tfidf_vectors, labels, test_size=0.2, random_state=42)\n","X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_vectors, labels, test_size=0.2, random_state=42)\n","\n","# 4. TF-IDF ile Random Forest modelini oluşturup eğitelim\n","rf_tfidf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 ağaçla eğitim yapıyoruz\n","rf_tfidf_classifier.fit(X_train_tfidf, y_train)\n","\n","# 5. Word2Vec ile Random Forest modelini oluşturup eğitelim\n","rf_w2v_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 ağaçla eğitim yapıyoruz\n","rf_w2v_classifier.fit(X_train_w2v, y_train)\n","\n","# 6. TF-IDF modelinin tahminlerini yapalım\n","y_pred_tfidf = rf_tfidf_classifier.predict(X_test_tfidf)\n","\n","# 7. Word2Vec modelinin tahminlerini yapalım\n","y_pred_w2v = rf_w2v_classifier.predict(X_test_w2v)\n","\n","# 8. TF-IDF modelini değerlendirelim\n","print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, y_pred_tfidf))  # Doğruluk oranı\n","print(\"TF-IDF Classification Report:\\n\", classification_report(y_test, y_pred_tfidf, zero_division=0))  # Diğer metrikler\n","\n","# 9. Word2Vec modelini değerlendirelim\n","print(\"Word2Vec Model Accuracy:\", accuracy_score(y_test, y_pred_w2v))  # Doğruluk oranı\n","print(\"Word2Vec Classification Report:\\n\", classification_report(y_test, y_pred_w2v, zero_division=0))  # Diğer metrikler\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCgKmwADA_rw","executionInfo":{"status":"ok","timestamp":1734731838678,"user_tz":-60,"elapsed":12237,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"9bfddac4-ee3a-46ff-a10a-5e2d7292b8b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Model Accuracy: 0.86\n","TF-IDF Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.82      0.84      0.83       120\n","           1       0.89      0.87      0.88       122\n","           2       0.87      0.87      0.87       158\n","\n","    accuracy                           0.86       400\n","   macro avg       0.86      0.86      0.86       400\n","weighted avg       0.86      0.86      0.86       400\n","\n","Word2Vec Model Accuracy: 0.6575\n","Word2Vec Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.58      0.47      0.52       120\n","           1       0.81      0.80      0.81       122\n","           2       0.60      0.68      0.64       158\n","\n","    accuracy                           0.66       400\n","   macro avg       0.66      0.65      0.66       400\n","weighted avg       0.66      0.66      0.65       400\n","\n"]}]},{"cell_type":"markdown","source":["# Naive Bayes Sınıflandırıcı\n","Önceki adımları takip ederek Naive Bayes eğittim."],"metadata":{"id":"LZ5AoGE_BY0I"}},{"cell_type":"code","source":["df_2 = df_copy.copy()"],"metadata":{"id":"VqzoQzUNB1ON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","# 1. TF-IDF ve Word2Vec vektörlerini numpy array formatına dönüştürme\n","tfidf_vectors = np.array(df_2['TFIDF_Vector'].tolist())  # TFIDF vektörlerini numpy array'e dönüştürme\n","word2vec_vectors = np.array(df_2['Word2Vec_Vector'].tolist())  # Word2Vec vektörlerini numpy array'e dönüştürme\n","\n","# 2. 'status' etiketini alalım ve Label Encoding yapalım\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(df_2['status'])  # Positive, Neutral gibi etiketleri sayısal hale getirme\n","\n","# 3. Veriyi eğitim ve test setlerine ayıralım\n","X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(tfidf_vectors, labels, test_size=0.2, random_state=42)\n","X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_vectors, labels, test_size=0.2, random_state=42)\n","\n","# 4. TF-IDF ile Naive Bayes modelini oluşturup eğitelim\n","nb_tfidf_classifier = GaussianNB()  # Gaussian Naive Bayes sınıflandırıcı\n","nb_tfidf_classifier.fit(X_train_tfidf, y_train)\n","\n","# 5. Word2Vec ile Naive Bayes modelini oluşturup eğitelim\n","nb_w2v_classifier = GaussianNB()  # Gaussian Naive Bayes sınıflandırıcı\n","nb_w2v_classifier.fit(X_train_w2v, y_train)\n","\n","# 6. TF-IDF modelinin tahminlerini yapalım\n","y_pred_tfidf = nb_tfidf_classifier.predict(X_test_tfidf)\n","\n","# 7. Word2Vec modelinin tahminlerini yapalım\n","y_pred_w2v = nb_w2v_classifier.predict(X_test_w2v)\n","\n","# 8. TF-IDF modelini değerlendirelim\n","print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, y_pred_tfidf))  # Doğruluk oranı\n","print(\"TF-IDF Classification Report:\\n\", classification_report(y_test, y_pred_tfidf, zero_division=0))  # Diğer metrikler\n","\n","# 9. Word2Vec modelini değerlendirelim\n","print(\"Word2Vec Model Accuracy:\", accuracy_score(y_test, y_pred_w2v))  # Doğruluk oranı\n","print(\"Word2Vec Classification Report:\\n\", classification_report(y_test, y_pred_w2v, zero_division=0))  # Diğer metrikler\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xn65W_hT7JD3","executionInfo":{"status":"ok","timestamp":1734731838885,"user_tz":-60,"elapsed":208,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"1f38dc37-9c0c-49b8-83ba-c6897b98b962"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Model Accuracy: 0.7575\n","TF-IDF Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.76      0.73      0.75       120\n","           1       0.65      0.93      0.77       122\n","           2       0.92      0.65      0.76       158\n","\n","    accuracy                           0.76       400\n","   macro avg       0.78      0.77      0.76       400\n","weighted avg       0.79      0.76      0.76       400\n","\n","Word2Vec Model Accuracy: 0.35\n","Word2Vec Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.31      0.33      0.32       120\n","           1       0.36      0.72      0.48       122\n","           2       0.48      0.08      0.14       158\n","\n","    accuracy                           0.35       400\n","   macro avg       0.38      0.38      0.31       400\n","weighted avg       0.39      0.35      0.30       400\n","\n"]}]},{"cell_type":"markdown","source":["# K-Nearest Neighbors Sınıflandırıcı\n","Önceki adımları takip ederek K-Nearest Neighbors eğittim."],"metadata":{"id":"JyIuWwWpB4wN"}},{"cell_type":"code","source":["df_2 = df_copy.copy()"],"metadata":{"id":"vY8X7_pGCLl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# 1. TF-IDF ve Word2Vec vektörlerini numpy array formatına dönüştürme\n","tfidf_vectors = np.array(df_2['TFIDF_Vector'].tolist())  # TFIDF vektörlerini numpy array'e dönüştürme\n","word2vec_vectors = np.array(df_2['Word2Vec_Vector'].tolist())  # Word2Vec vektörlerini numpy array'e dönüştürme\n","\n","# 2. 'status' etiketini alalım ve Label Encoding yapalım\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(df_2['status'])  # Positive, Neutral gibi etiketleri sayısal hale getirme\n","\n","# 3. Veriyi eğitim ve test setlerine ayıralım\n","X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(tfidf_vectors, labels, test_size=0.2, random_state=42)\n","X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_vectors, labels, test_size=0.2, random_state=42)\n","\n","# 4. TF-IDF ile KNN modelini oluşturup eğitelim\n","knn_tfidf_classifier = KNeighborsClassifier(n_neighbors=5)  # KNN modelinde k=5 komşu kullanıyoruz\n","knn_tfidf_classifier.fit(X_train_tfidf, y_train)\n","\n","# 5. Word2Vec ile KNN modelini oluşturup eğitelim\n","knn_w2v_classifier = KNeighborsClassifier(n_neighbors=5)  # KNN modelinde k=5 komşu kullanıyoruz\n","knn_w2v_classifier.fit(X_train_w2v, y_train)\n","\n","# 6. TF-IDF modelinin tahminlerini yapalım\n","y_pred_tfidf = knn_tfidf_classifier.predict(X_test_tfidf)\n","\n","# 7. Word2Vec modelinin tahminlerini yapalım\n","y_pred_w2v = knn_w2v_classifier.predict(X_test_w2v)\n","\n","# 8. TF-IDF modelini değerlendirelim\n","print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, y_pred_tfidf))  # Doğruluk oranı\n","print(\"TF-IDF Classification Report:\\n\", classification_report(y_test, y_pred_tfidf, zero_division=1))  # Diğer metrikler\n","\n","# 9. Word2Vec modelini değerlendirelim\n","print(\"Word2Vec Model Accuracy:\", accuracy_score(y_test, y_pred_w2v))  # Doğruluk oranı\n","print(\"Word2Vec Classification Report:\\n\", classification_report(y_test, y_pred_w2v, zero_division=1))  # Diğer metrikler\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlJNtyMB7oYF","executionInfo":{"status":"ok","timestamp":1734731839401,"user_tz":-60,"elapsed":517,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"04980df9-a45a-4422-bfff-7a9b75abc669"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Model Accuracy: 0.7125\n","TF-IDF Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.88      0.53      0.66       120\n","           1       0.56      0.93      0.70       122\n","           2       0.88      0.68      0.76       158\n","\n","    accuracy                           0.71       400\n","   macro avg       0.77      0.71      0.71       400\n","weighted avg       0.78      0.71      0.71       400\n","\n","Word2Vec Model Accuracy: 0.4625\n","Word2Vec Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.36      0.38      0.37       120\n","           1       0.70      0.49      0.58       122\n","           2       0.42      0.51      0.46       158\n","\n","    accuracy                           0.46       400\n","   macro avg       0.49      0.46      0.47       400\n","weighted avg       0.49      0.46      0.47       400\n","\n"]}]},{"cell_type":"markdown","source":["# LSTM Modeli\n","Önceki adımları takip ederek Long Short Term Memory modeli eğittim."],"metadata":{"id":"Vj9wRNL8CgUk"}},{"cell_type":"code","source":["df_2 = df_copy.copy()"],"metadata":{"id":"3FsNMM3yDA2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# 1. TF-IDF ve Word2Vec vektörlerini numpy array formatına dönüştürme\n","tfidf_vectors = np.array(df_2['TFIDF_Vector'].tolist())  # TFIDF vektörlerini numpy array'e dönüştürme\n","word2vec_vectors = np.array(df_2['Word2Vec_Vector'].tolist())  # Word2Vec vektörlerini numpy array'e dönüştürme\n","\n","# 2. 'status' etiketini alalım ve Label Encoding yapalım\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(df_2['status'])  # Positive, Neutral gibi etiketleri sayısal hale getirme\n","\n","# 3. Veriyi eğitim ve test setlerine ayıralım\n","X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(tfidf_vectors, labels, test_size=0.2, random_state=42)\n","X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_vectors, labels, test_size=0.2, random_state=42)\n","\n","# 4. LSTM modelini hazırlamak için giriş verilerini pad'lemek\n","# Pad edilmesi gerekebilir çünkü LSTM, sabit boyutta girişler ister\n","max_sequence_length = max([len(x) for x in tfidf_vectors])  # Verinin en uzun boyutunu alıyoruz\n","X_train_tfidf = pad_sequences(X_train_tfidf, maxlen=max_sequence_length)\n","X_test_tfidf = pad_sequences(X_test_tfidf, maxlen=max_sequence_length)\n","\n","X_train_w2v = pad_sequences(X_train_w2v, maxlen=max_sequence_length)\n","X_test_w2v = pad_sequences(X_test_w2v, maxlen=max_sequence_length)\n","\n","# 5. TF-IDF verisi için LSTM modelini oluşturup eğitelim\n","lstm_tfidf_model = Sequential()\n","lstm_tfidf_model.add(LSTM(64, activation='relu', input_shape=(max_sequence_length, 1), return_sequences=False))\n","lstm_tfidf_model.add(Dropout(0.5))\n","lstm_tfidf_model.add(Dense(32, activation='relu'))\n","lstm_tfidf_model.add(Dense(len(np.unique(labels)), activation='softmax'))  # sınıf sayısına göre çıkış\n","\n","lstm_tfidf_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# LSTM için veriyi 3D yapmamız gerekiyor (num_samples, max_sequence_length, 1)\n","X_train_tfidf_reshaped = np.expand_dims(X_train_tfidf, axis=-1)\n","X_test_tfidf_reshaped = np.expand_dims(X_test_tfidf, axis=-1)\n","\n","lstm_tfidf_model.fit(X_train_tfidf_reshaped, y_train, epochs=5, batch_size=64, validation_data=(X_test_tfidf_reshaped, y_test))\n","\n","# 6. Word2Vec verisi için LSTM modelini oluşturup eğitelim\n","lstm_w2v_model = Sequential()\n","lstm_w2v_model.add(LSTM(64, activation='relu', input_shape=(max_sequence_length, 1), return_sequences=False))\n","lstm_w2v_model.add(Dropout(0.5))\n","lstm_w2v_model.add(Dense(32, activation='relu'))\n","lstm_w2v_model.add(Dense(len(np.unique(labels)), activation='softmax'))  # sınıf sayısına göre çıkış\n","\n","lstm_w2v_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Word2Vec için veriyi 3D yapmamız gerekiyor (num_samples, max_sequence_length, 1)\n","X_train_w2v_reshaped = np.expand_dims(X_train_w2v, axis=-1)\n","X_test_w2v_reshaped = np.expand_dims(X_test_w2v, axis=-1)\n","\n","lstm_w2v_model.fit(X_train_w2v_reshaped, y_train, epochs=5, batch_size=64, validation_data=(X_test_w2v_reshaped, y_test))\n","\n","# 7. TF-IDF modelinin tahminlerini yapalım\n","y_pred_tfidf = lstm_tfidf_model.predict(X_test_tfidf_reshaped)\n","y_pred_tfidf = np.argmax(y_pred_tfidf, axis=1)  # En yüksek olasılığa sahip sınıfı seçiyoruz\n","\n","# 8. Word2Vec modelinin tahminlerini yapalım\n","y_pred_w2v = lstm_w2v_model.predict(X_test_w2v_reshaped)\n","y_pred_w2v = np.argmax(y_pred_w2v, axis=1)  # En yüksek olasılığa sahip sınıfı seçiyoruz\n","\n","# 9. TF-IDF modelini değerlendirelim\n","print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, y_pred_tfidf))  # Doğruluk oranı\n","print(\"TF-IDF Classification Report:\\n\", classification_report(y_test, y_pred_tfidf, zero_division=1))  # Diğer metrikler\n","\n","# 10. Word2Vec modelini değerlendirelim\n","print(\"Word2Vec Model Accuracy:\", accuracy_score(y_test, y_pred_w2v))  # Doğruluk oranı\n","print(\"Word2Vec Classification Report:\\n\", classification_report(y_test, y_pred_w2v, zero_division=1))  # Diğer metrikler\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3gFsPw1CZvh","executionInfo":{"status":"ok","timestamp":1734731913884,"user_tz":-60,"elapsed":74484,"user":{"displayName":"Eda","userId":"04149009669729752691"}},"outputId":"3eaefcd2-9b8b-4327-c8ec-2758fa1bb284"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 284ms/step - accuracy: 0.3183 - loss: 1.0985 - val_accuracy: 0.3950 - val_loss: 1.0963\n","Epoch 2/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.3690 - loss: 1.0973 - val_accuracy: 0.3950 - val_loss: 1.0940\n","Epoch 3/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - accuracy: 0.3828 - loss: 1.0948 - val_accuracy: 0.3950 - val_loss: 1.0919\n","Epoch 4/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.3803 - loss: 1.0939 - val_accuracy: 0.3950 - val_loss: 1.0916\n","Epoch 5/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.3619 - loss: 1.0969 - val_accuracy: 0.3950 - val_loss: 1.0915\n","Epoch 1/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - accuracy: 0.3687 - loss: 1.0983 - val_accuracy: 0.3950 - val_loss: 1.0971\n","Epoch 2/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 203ms/step - accuracy: 0.3788 - loss: 1.0974 - val_accuracy: 0.3950 - val_loss: 1.0958\n","Epoch 3/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.3845 - loss: 1.0961 - val_accuracy: 0.3950 - val_loss: 1.0946\n","Epoch 4/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - accuracy: 0.3822 - loss: 1.0956 - val_accuracy: 0.3950 - val_loss: 1.0939\n","Epoch 5/5\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.3761 - loss: 1.0955 - val_accuracy: 0.3950 - val_loss: 1.0933\n","\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n","\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n","TF-IDF Model Accuracy: 0.395\n","TF-IDF Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.00      0.00       120\n","           1       1.00      0.00      0.00       122\n","           2       0.40      1.00      0.57       158\n","\n","    accuracy                           0.40       400\n","   macro avg       0.80      0.33      0.19       400\n","weighted avg       0.76      0.40      0.22       400\n","\n","Word2Vec Model Accuracy: 0.395\n","Word2Vec Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.00      0.00       120\n","           1       1.00      0.00      0.00       122\n","           2       0.40      1.00      0.57       158\n","\n","    accuracy                           0.40       400\n","   macro avg       0.80      0.33      0.19       400\n","weighted avg       0.76      0.40      0.22       400\n","\n"]}]},{"cell_type":"markdown","source":["# Sonuç\n","\n","Farklı modellerle yapılan denemelerde, TF-IDF ve Word2Vec tekniklerinin başarı oranları aşağıdaki gibi bulunmuştur.\n","\n","|                     | TF-IDF | Word2Vec |\n","|---------------------|--------|----------|\n","| Logistic Regression | 0.875  | 0.365    |\n","| SVM                 | 0.8725 | 0.395    |\n","| Random Forest       | 0.86   | 0.6575   |\n","| Naive Bayes         | 0.7575 | 0.35     |\n","| K-Nearest Neighbors | 0.7125 | 0.4625   |\n","| LSTM                | 0.395  | 0.395    |\n","\n","Yapılan 6 testin beşinde **TF-IDF** tekniği kullanılarak eğitilen model, Word2Vec ile eğitilen modelden önemli ölçüde daha başarılı olmuştur. Dolayısıyla metinlerde duygu analizi yapmak için TF-IDF'in daha uygun bir vektörleştirme yöntemi olduğu söylenebilir. Test edilen modeller arasında ise **Logistic Regression** ve **SVM** en başarılı sonuçları vermiştir. Ancak modeller eğitilirken hiperparametre optimizasyonu yapılmamıştır. Dolayısıyla, en uygun parametrelerle eğitildiğinde modellerin başarı oranları değişebilir."],"metadata":{"id":"TmSX0Ll8OOAQ"}}]}